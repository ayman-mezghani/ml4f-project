{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Goal: predict price returns across a bundle of major 14 cryptocurrencies  in the time scale of minutes to hours.\n",
    "\n",
    "Evaluation: correlation with real market data collected during the three-month evaluation period after the competition has closed.\n",
    "The predictions will be evaluated on a weighted version of the Pearson correlation coefficient, with weights given by the Weight column in the Asset Details file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Useful links:\n",
    "* Tutorial to the G-Research Crypto Competition\n",
    "https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition#Crypto-forecasting-tutorial\n",
    "* G-Research Crypto Forecasting EDA\n",
    "https://www.kaggle.com/iamleonie/to-the-moon-g-research-crypto-forecasting-eda#Data-Overview\n",
    "* Time Series: Interpreting ACF and PACF https://www.kaggle.com/iamleonie/time-series-interpreting-acf-and-pacf\n",
    "* Hyperparam tune https://www.kaggle.com/junjitakeshima/crypto-beginner-s-try-for-simple-lgbm-en-jp\n",
    "* Complex models: https://www.kaggle.com/yamqwe/time-series-modeling-n-beats\n",
    "* Datart https://www.kaggle.com/cecileguillot/naive-drift-and-regression-approaches\n",
    "    * Market: crypto-exchanges, with an average volume of $41 billion traded daily over the last year, according to CryptoCompare (as of 25th July 2021),  the market and meme manipulation, the correlation between assets and the very fast changing market conditions\n",
    "    * Data Overview: changes in prices between different cryptocurrencies are highly interconnected, extreme volatility of the assets, non-stationary\n",
    "   From 2018-01-01 to 2021-09-21 for the majority of coins. For TRON, Stellar, Cardano, IOTA, Maker, and Dogecoin we have fewer data starting from later in 2018 or even later in 2019 in Dogecoin's case.\n",
    "    * Data features description:\n",
    "        * timestamp: All timestamps are returned as second Unix timestamps (the number of seconds elapsed since 1970-01-01 00:00:00.000 UTC). Timestamps in this dataset are multiple of 60, indicating minute-by-minute data.\n",
    "        * Asset_ID\n",
    "        * Count: Total number of trades in the time interval (last minute).\n",
    "        * Open: Opening price of the time interval (in USD).\n",
    "        * High: Highest price reached during time interval (in USD).\n",
    "        * Low: Lowest price reached during time interval (in USD).\n",
    "        * Close: Closing price of the time interval (in USD).\n",
    "        * Volume: Quantity of asset bought or sold, displayed in base currency USD.\n",
    "        * VWAP: The average price of the asset over the time interval, weighted by volume. VWAP is an aggregated form of trade data.\n",
    "        * Target: Residual log-returns for the asset over a 15 minute horizon.\n",
    "           Predict returns in the near future for prices $P^a$, for each asset $a$. $$R^a(t) = log (P^a(t+16)\\ /\\ P^a(t+1))$$\n",
    "    * EDA:\n",
    "        * Missing asset data, for a given minute, is not represented by NaN's, but instead by the absence of those rows.\n",
    "      there are many gaps in the data. To work with most time series models,the preprocess of data into a format without time gaps should be done. Missing data should be dropped\n",
    "        * Autocorrelation: The 'Close' prices seem to be mostly non-stationary. However, Bitcoin and Ethereum seem to be stationary. -> to make it stationary log_return - The log returns seem to be stationary\n",
    "        * For the majority of the coins it looks like we have a slight negative correlation at a lag of 1. For the other lags, it looks like the autocorrelations are statistically insignificant. This could indicate, that we have some Random-Walk behavior, which is going to make the prediction challenging.\n",
    "        * Correlation btc and eth:  high but variable correlation between the assets over time. correlation has proven difficult to maintain. For example, bitcoin prices fell even as prices for Ethereumâ€™s ether (ETH) rose to new heights in early 2018.\n",
    "        * Trend: negative peak around March 2020\n",
    "* TO DO:\n",
    "    * Plot with diff moving average\n",
    "    * Add more data feat:\n",
    "        * given time-series of historical prices  = millions of rows of minute-by-minute cryptocurrency trading data\n",
    "        * decompose date (timestamp)\n",
    "        * upper/ lower shadow\n",
    "        * use quora, reddit, twitter, EDGAR Data for NLP\n",
    "        * reduce the data memory ?\n",
    "        * https://www.kaggle.com/yamqwe/time-series-modeling-n-beats\n",
    "    * Stat tests:\n",
    "        * autocorrelation, time-series decomposition and stationarity tests.\n",
    "    * EDA: relationship between the different coins (close/ avg/ open per day) depending on the year,\n",
    "    * Train_test split with diff time frames / paddles ; diff windows\n",
    "    * Add more Preprocess\n",
    "        * Standardization StandardScaler(), MinMax\n",
    "        * Sliding windows over a timeseries https://keras.io/api/preprocessing/timeseries/\n",
    "    * Model:\n",
    "        * Baseline model: Linear Regression, MultiOutputRegressor(LinearRegression()), Ridge()\n",
    "        * LGBMRegressor\n",
    "            * lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"n_estimators\" : 500,     # <-- (9) change from 200 to 500\n",
    "    \"num_leaves\" : 300,       # <-- (10) Added parameter\n",
    "    \"learning_rate\" : 0.09,   # <-- (10) Added parameter\n",
    "    \"random_seed\" : 1234}\n",
    "            * change use of model from \"for all data\" to \"for each asset\"\n",
    "    * N-BEATS https://arxiv.org/pdf/1905.10437.pdf https://www.kaggle.com/yamqwe/time-series-modeling-n-beats\n",
    "    * SARIMA, AR, MA, ARMA, ARIMA https://towardsdatascience.com/time-series-models-d9266f8ac7b0\n",
    "        * from darts.models import (\n",
    "    NaiveSeasonal,\n",
    "    NaiveDrift,\n",
    "    Prophet,\n",
    "    ExponentialSmoothing,\n",
    "    ARIMA,\n",
    "    AutoARIMA,\n",
    "    RegressionEnsembleModel,\n",
    "    RegressionModel,\n",
    "    FFT\n",
    ")\n",
    "    * Recurrent neural networks (RNN)\n",
    "    * RNN variants (LSTM, GRU) https://keras.io/api/layers/recurrent_layers/\n",
    "    * TCN https://github.com/philipperemy/keras-tcn\n",
    "    *\n",
    "(week 6)\n",
    "    * val by weighted correlation\n",
    "    * Tune hyperparams\n",
    "    * Submission: Note that this is a Code Competition,\n",
    "  in which you must submit your notebook to be run against the hidden private data.\n",
    "  Your notebook should use the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow the instructions and template in Code Competition Detailed API instructions and Basic Submission Template.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}