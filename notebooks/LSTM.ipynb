{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0680501-da20-43bb-9da8-5e209ae4ece8",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b246c-d9dd-4f5c-88cb-4e84ee3db1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "import gc\n",
    "#from datetime import datetime\n",
    "#import time\n",
    "\n",
    "#import math\n",
    "#from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from statsmodels.tsa.ar_model import AutoReg\n",
    "#from statsmodels.tsa.arima_model import ARMA, ARIMA\n",
    "#from statsmodels.tsa.stattools import adfuller\n",
    "#from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "#from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "#from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "#import statsmodels.api as sm\n",
    "\n",
    "#from sklearn.model_selection import KFold\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "#from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# https://github.com/philipperemy/keras-tcn\n",
    "#from tcn import TCN\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "#from plotly.subplots import make_subplots\n",
    "#import plotly.express as px\n",
    "#import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6163b151-f722-46b9-88d1-cfa4446e5b07",
   "metadata": {},
   "source": [
    "tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e73de05-51b4-4e54-ab83-c3a6fc250749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = '/kaggle/input/'\n",
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d69980-78a8-47e7-bb45-7fe94134d6f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Artificial Neural Networks Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0d991-9151-4e9d-a92a-f24b11241292",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "N_ASSETS = 1\n",
    "WINDOW_SIZE = 15\n",
    "BATCH_SIZE = 1024\n",
    "PCT_VALIDATION = 10 # last 10% of the data are used as validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5f115-7a66-4a4e-a615-fe01c14be86d",
   "metadata": {},
   "source": [
    "### Data Preparation (Building the Time Series Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc34f2-e90f-4168-81ac-3753ebdf154b",
   "metadata": {},
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994136e-d11d-439b-83ec-f320de1b8de1",
   "metadata": {},
   "source": [
    "Loading data and converting timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b6775-999a-41c9-9667-7066bcd82b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_details = pd.read_csv(data_path + 'g-research-crypto-forecasting/asset_details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005002d5-efc0-4ada-993e-832fc7b16a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_details.sort_values(by='Asset_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb3ce5-8f92-45c0-9feb-ad3a2b7c1a3d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path + \"g-research-crypto-forecasting/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e89a9-dfd9-4249-8a32-9387a1602a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823846d-6193-4a1c-b2a8-b323e1b17eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.Asset_ID == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33483c-51ea-41a0-b584-26c882cf15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'], unit='s')\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc8234-d4f4-49a7-9611-c7f81ba62a48",
   "metadata": {},
   "source": [
    "#### Reindexing and sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d252ce-6fce-4409-8182-86e85990dd48",
   "metadata": {},
   "source": [
    "Asset-wise reindexing, performing a forward fill when possible, else a backfill. We create a column `is_real` to note whether a row is existant in the original dataset or added due to reindexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5afb1-9378-4a6f-bcf9-0a60308ef1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe target should not be filled. The filling has to be re-examined"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7504397-82eb-4303-b0c6-f21adfd18018",
   "metadata": {},
   "source": [
    "train.index = train.timestamp\n",
    "train.sort_index(inplace=True)\n",
    "train['is_real'] = True\n",
    "#train['existing_target'] = ~train.Target.isna()\n",
    "ind = train.index.unique()\n",
    "\n",
    "def reindex(df):\n",
    "    res = df.reindex(pd.date_range(ind.min(), ind.max(), freq='min'))\n",
    "    res['is_real'].fillna(False, inplace=True)\n",
    "    #res['existing_target'].fillna(True, inplace=True)\n",
    "    res['timestamp'] = res.index\n",
    "    res = res.fillna(method=\"ffill\")\n",
    "    #res['Target'] = res.Target * res.existing_target.apply(lambda x: 1 if x else np.nan)\n",
    "    return res\n",
    "\n",
    "train = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_values(by=['timestamp', 'Asset_ID'])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75088259-1545-4979-9172-85d51646a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.index = train.timestamp\n",
    "train.sort_index(inplace=True)\n",
    "train['is_real'] = True\n",
    "ind = train.index.unique()\n",
    "\n",
    "def reindex(df):\n",
    "    res = df.reindex(pd.date_range(ind.min(), ind.max(), freq='min'))\n",
    "    res['is_real'].fillna(False, inplace=True)\n",
    "    res['timestamp'] = res.index\n",
    "    res = res.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    return res\n",
    "\n",
    "train = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_values(by=['timestamp', 'Asset_ID'])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc48ef80-a30c-4a92-8e2b-01e600633786",
   "metadata": {},
   "source": [
    "# Smaller dataset for debugging\n",
    "if DEBUG:\n",
    "    train = train[:500010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3dc88a-f718-4772-acaa-e5335cc728d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbbbcf0-e01e-4b5d-8d94-89d38c29062b",
   "metadata": {},
   "source": [
    "#### VWAP clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4c817-c5c9-4bb1-9889-63c0843d1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "VWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\n",
    "VWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\n",
    "train['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad42312-9d9c-4352-8702-0cdeb1865f43",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c18441-c8d0-4ec3-ae2b-554ff1d1037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(_df, row = False):\n",
    "    #_df = df.copy()\n",
    "    \n",
    "    _df['Spread'] = _df['High'] - _df['Low']\n",
    "    _df['Close-Open'] = _df['Close'] - _df['Open']\n",
    "\n",
    "    _df['Upper_Shadow'] = _df['High'] - np.maximum(_df['Close'], _df['Open'])\n",
    "    _df['Lower_Shadow'] = np.minimum(_df['Close'], _df['Open']) - _df['Low']\n",
    "    \n",
    "    _df['High/Low'] = _df['High'] / _df['Low']\n",
    "    _df['log_High/Low'] = np.log(_df['High/Low'])\n",
    "    \n",
    "    _df['Close/Open'] = _df['Close'] / _df['Open']\n",
    "    _df['log_Close/Open'] = np.log(_df['Close/Open'])\n",
    "\n",
    "    _df['Volume/Count'] = _df['Volume'] / (_df['Count'] + 1)\n",
    "\n",
    "    _df['LOGVOL'] = np.log(1. + _df['Volume'])\n",
    "    _df['LOGCNT'] = np.log(1. + _df['Count'])\n",
    "\n",
    "    _df['Mean'] = _df[['Open', 'High', 'Low', 'Close']].mean(axis = 1)\n",
    "    _df['High/Mean'] = _df['High'] / _df['Mean']\n",
    "    _df['Low/Mean'] = _df['Low'] / _df['Mean']\n",
    "\n",
    "    _df['Median'] = _df[['Open', 'High', 'Low', 'Close']].median(axis=1)\n",
    "    _df['High/Median'] = _df['High'] / _df['Median']\n",
    "    _df['Low/Median'] = _df['Low'] / _df['Median']\n",
    "\n",
    "    \"\"\"################################### ???\n",
    "\n",
    "        _df['RNG'] = (_df['High'] - _df['Low']) / _df['VWAP']\n",
    "        _df['MOV'] = (_df['Close'] - _df['Open']) / _df['VWAP']\n",
    "        _df['CLS'] = (_df['Close'] - _df['VWAP']) / _df['VWAP']\n",
    "\n",
    "    ################################### ???\n",
    "\n",
    "        _df['gtrade'] = _df['Close-Open'] / _df['Count']\n",
    "        _df['shadow1'] = _df['Close-Open'] / (_df['Volume'] + 1)\n",
    "        _df['shadow3'] = _df['Upper_Shadow'] / (_df['Volume'] + 1)\n",
    "        _df['shadow5'] = _df['Lower_Shadow'] / (_df['Volume'] + 1)\n",
    "\n",
    "        _df['diff1'] = _df['Volume'] - _df['Count'] # ?????\n",
    "\n",
    "        _df['mean1'] = (_df['shadow5'] + _df['shadow3']) / 2\n",
    "        _df['mean2'] = (_df['shadow1'] + _df['Volume']) / 2\n",
    "        _df['mean3'] = (_df['Close-Open'] + _df['gtrade']) / 2\n",
    "        _df['mean4'] = (_df['diff1'] + _df['Upper_Shadow']) / 2\n",
    "        _df['mean5'] = (_df['diff1'] + _df['Lower_Shadow']) / 2\"\"\"\n",
    "\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047d0bf-bc1a-4b8b-a397-00303ff511bb",
   "metadata": {},
   "source": [
    "Get the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cef8e9-bd10-4f58-9769-a2ef7738bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_eng(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60664ddf-ef93-4381-be27-6c62773ff74e",
   "metadata": {},
   "source": [
    "some params needed for the model, sampling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69fc49-eee1-4a20-9946-7a1464883af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train['Target'].to_numpy().reshape(-1, N_ASSETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb30a1b-402c-486f-ab0a-bca7cac55d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = train.columns.drop(['Asset_ID', 'Target', 'timestamp', 'is_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a609a8a-555b-46da-9bee-82efe141ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734069e-5e69-4d1f-b001-460b3abdf284",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3908b-9a7e-447e-b469-ca7f10a370c3",
   "metadata": {},
   "source": [
    "#### Managing added timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4d1ea-aaef-425f-ad97-9003574db230",
   "metadata": {},
   "source": [
    "non-real data features are set to 0 which are then masked by the model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a990f8d-7325-4c53-91e5-a4908733455b",
   "metadata": {},
   "source": [
    "for col in feature_cols:\n",
    "    train[col] = train[col] * train.is_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da9b2f-786a-4ff8-9f93-76b6c37075ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c5ae0d-9fc2-48f8-8f72-c6101191ebdf",
   "metadata": {},
   "source": [
    "#### Generating training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488df09c-437f-4dc0-baf0-5fc905143446",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[feature_cols].values\n",
    "train_data = train_data.reshape(-1, N_ASSETS, train_data.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87044fdf-5faf-4abe-bbf6-26abc3a1c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821f7a3-806c-493e-9d22-315cd17e4f2b",
   "metadata": {},
   "source": [
    "Samples with a duration of WINDOW_SIZE records (minutes) will be formed from the train array. Each sample has a target vector corresponding to the final index if WINDOW_SIZE record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9b938-3c62-4a36-a6ce-aafeba7bc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_generator(keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, length):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.length = length\n",
    "        self.size = len(x_set)\n",
    "    def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for i in range(self.batch_size):\n",
    "            start_ind = self.batch_size * idx + i\n",
    "            end_ind = start_ind + self.length \n",
    "            if end_ind <= self.size:\n",
    "                batch_x.append(self.x[start_ind : end_ind])\n",
    "                batch_y.append(self.y[end_ind -1])\n",
    "        return np.array(batch_x), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a6125-a52f-40e9-969a-4d3fd9739f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_data[:-len(train_data)//PCT_VALIDATION], train_data[-len(train_data)//PCT_VALIDATION:]\n",
    "y_train, y_test = targets[:-len(train_data)//PCT_VALIDATION], targets[-len(train_data)//PCT_VALIDATION:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82fd33-8e0c-4bb7-aa3d-eacb041aceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = sample_generator(X_train, y_train, length=WINDOW_SIZE, batch_size=BATCH_SIZE)\n",
    "val_generator = sample_generator(X_test, y_test, length=WINDOW_SIZE, batch_size=BATCH_SIZE)\n",
    "print(f'Sample shape: {train_generator[0][0].shape}')\n",
    "print(f'Target shape: {train_generator[0][1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1f237-6e5c-496e-8a47-7f988891ca82",
   "metadata": {},
   "source": [
    "### Metrics and Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92ae0d-2bef-423c-8863-f376da577d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlations for predicted and real\n",
    "def MaxCorrelation(y_true,y_pred): \n",
    "    return -tf.math.abs(tfp.stats.correlation(y_pred, y_true, sample_axis=None, event_axis=None))\n",
    "\n",
    "def Correlation(y_true,y_pred): \n",
    "    return tf.math.abs(tfp.stats.correlation(y_pred, y_true, sample_axis=None, event_axis=None))\n",
    "\n",
    "#Masked losses\n",
    "def masked_mse(y_true, y_pred):\n",
    "    mask = tf.math.not_equal(y_true, 0.)\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    return tf.keras.losses.mean_squared_error(y_true=y_true_masked, y_pred=y_pred_masked)\n",
    "\n",
    "def masked_mae(y_true, y_pred):\n",
    "    mask = tf.math.not_equal(y_true, 0.)\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    return tf.keras.losses.mean_absolute_error(y_true=y_true_masked, y_pred=y_pred_masked)\n",
    "\n",
    "def masked_cosine(y_true, y_pred):\n",
    "    mask = tf.math.not_equal(y_true, 0.)\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    return tf.keras.losses.cosine_similarity(y_true_masked, y_pred_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107770c6-f72b-4ff0-ada9-9188f511763b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff68de-35b7-4611-ac89-98f308e5bed8",
   "metadata": {},
   "source": [
    "#### Multivariate 1-Layered LSTM with Gloobal Average Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ab6f1-5bfd-4876-84a0-506219875f9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Model\n",
    "Our model will be trained for the number of FOLDS and EPOCHS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variable VERBOSE. The variable VERBOSE=1 or 2 will display the training and validation loss for each epoch as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a288a9a-b147-4e01-a76b-88e4aec1d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "def get_model(n_assets=N_ASSETS):\n",
    "    x_input = keras.Input(shape=(train_generator[0][0].shape[1], n_assets, train_generator[0][0].shape[-1]))\n",
    "    branch_outputs = []\n",
    "    \n",
    "    for i in range(n_assets):\n",
    "        a = layers.Lambda(lambda x: x[:,:, i])(x_input) # Slicing the ith asset:\n",
    "        a = layers.Masking(mask_value=0.)(a)\n",
    "        a = tf.keras.layers.BatchNormalization()(a)\n",
    "        a = layers.LSTM(units=32, return_sequences=True)(a)\n",
    "        #a = layers.GlobalAvgPool1D()(a)\n",
    "        branch_outputs.append(a)\n",
    "        \n",
    "    x = layers.Concatenate()(branch_outputs)\n",
    "    x = layers.Dense(units=128)(x)\n",
    "    out = layers.Dense(units=n_assets)(x)\n",
    "    model = keras.Model(inputs=x_input, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=masked_mae, metrics=[Correlation])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16fafa-24ed-42a7-afa3-857b8323a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(get_model(n_assets=1), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635afe76-6691-4041-9888-5e9397d25104",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "estop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=0, mode='min', restore_best_weights=True)\n",
    "scheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5 * len(X_train) / BATCH_SIZE), 1e-3)\n",
    "lr = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bac315-782e-4049-8f8b-bfc6dbd23bd3",
   "metadata": {},
   "source": [
    "##### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06034e-0e28-4cd6-a3a9-a1cfae6b39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "history = model.fit(train_generator, validation_data=(val_generator), epochs=epochs, callbacks=[lr, estop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd5dfb-896f-449e-bb8b-9df35ba9c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    histories = pd.DataFrame(history.history)\n",
    "    epochs = list(range(1, len(histories)+1))\n",
    "    loss = histories['loss']\n",
    "    val_loss = histories['val_loss']\n",
    "    correlation = histories['Correlation']\n",
    "    val_correlation = histories['val_Correlation']\n",
    "    ax[0].plot(epochs, loss, label='Train Loss')\n",
    "    ax[0].plot(epochs, val_loss, label='Val Loss')\n",
    "    ax[0].set_title('Losses')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(loc='upper right')\n",
    "    ax[1].plot(epochs, correlation, label='Train Correlation')\n",
    "    ax[1].plot(epochs, val_correlation, label='Val Correlation')\n",
    "    ax[1].set_title('Correlations')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(loc='upper right')\n",
    "    fig.show()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d889d-0273-4017-931f-40ef8cfa4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def79703-1e6d-4819-b2da-b753a9c5b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2a688-1f23-4f40-9dd9-7a72b34c8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_details(predictions, y_test, asset_details, assets):\n",
    "    print('Asset:    Corr. coef.')\n",
    "    print('---------------------')\n",
    "    for i, asset in enumerate(assets):\n",
    "        # drop first 14 values in the y_test, since they are absent in val_generator labels\n",
    "        y_true = np.squeeze(y_test[WINDOW_SIZE - 1:, i])\n",
    "        y_pred = np.squeeze(predictions[:, i])\n",
    "        real_target_ind = np.argwhere(y_true != 0)\n",
    "        asset_name = asset_details[asset_details.Asset_ID == asset]['Asset_Name'].item()\n",
    "        print(f\"{asset_name}: {np.corrcoef(y_pred[real_target_ind].flatten(), y_true[real_target_ind].flatten())[0, 1]:.4f}\")\n",
    "        plt.plot(y_true, label='Target')\n",
    "        plt.plot(y_pred, label='Prediction')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Target')\n",
    "        plt.title(asset_name)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196d264-1b8a-4e9e-a272-a687820dd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_details(predictions=predictions, y_test=y_test, asset_details=asset_details, assets=train.Asset_ID.astype(int).unique())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f13eebe-0b9b-44a6-b58e-e624b78f6528",
   "metadata": {},
   "source": [
    "model.save('models/lstm/lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141ba8b-a695-4adf-905d-acf562a2156d",
   "metadata": {},
   "source": [
    "#### Multivariate 2-Layered Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da5417-8442-4b2b-948c-bc41b5b0b468",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa6e3c-66d9-46a5-8fc5-4904b94f4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "def get_model(n_assets=N_ASSETS):\n",
    "    x_input = keras.Input(shape=(train_generator[0][0].shape[1], n_assets, train_generator[0][0].shape[-1]))\n",
    "    branch_outputs = []\n",
    "\n",
    "    for i in range(n_assets):\n",
    "        a = layers.Lambda(lambda x: x[:,:, i])(x_input) # Slicing the ith asset:\n",
    "        a = layers.Masking(mask_value=0.)(a)\n",
    "        a = tf.keras.layers.BatchNormalization()(a)\n",
    "        a = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(a)\n",
    "        a = layers.Bidirectional(layers.LSTM(16))(a)\n",
    "        #a = layers.GlobalAvgPool1D()(a)\n",
    "        branch_outputs.append(a)\n",
    "    \n",
    "    x = layers.Concatenate()(branch_outputs)\n",
    "    x = layers.Dense(units = 128)(x)\n",
    "    out = layers.Dense(units = n_assets)(x)\n",
    "    model = keras.Model(inputs=x_input, outputs=out)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), loss=masked_mse, metrics=[Correlation])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d766e0-2815-4f8a-b358-533e4e1efb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(get_model(n_assets=1), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d622f-a662-47a0-a867-5f20c64ea2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "estop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=0, mode='min', restore_best_weights=True)\n",
    "scheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5 * len(X_train) / BATCH_SIZE), 1e-3)\n",
    "lr = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e7afc-6d23-4371-a354-1182eed49563",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466e598-1760-429b-843c-01af4722595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "history = model.fit(train_generator, validation_data=(val_generator), epochs=epochs, callbacks=[lr, estop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137cec3b-e36e-4b48-b628-1ecf0e7be725",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829f1a6-d2e0-43e2-b98d-985ae31edab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad27548-e76a-4c33-8884-05e7ad060930",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_details(predictions=predictions, y_test=y_test, asset_details=asset_details, assets=range(N_ASSETS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
